{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "plt.imshow(x_test[12], cmap='binary')\n",
    "plt.axis('off')\n",
    "print(y_test[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34826 (136.04 KB)\n",
      "Trainable params: 34826 (136.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 09:55:42.366901: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 169344000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 37s 84ms/step - loss: 0.3494 - accuracy: 0.8960 - val_loss: 0.0790 - val_accuracy: 0.9797\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 33s 78ms/step - loss: 0.1078 - accuracy: 0.9675 - val_loss: 0.0565 - val_accuracy: 0.9853\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 33s 78ms/step - loss: 0.0832 - accuracy: 0.9746 - val_loss: 0.0444 - val_accuracy: 0.9878\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0687 - accuracy: 0.9790 - val_loss: 0.0417 - val_accuracy: 0.9878\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0599 - accuracy: 0.9812 - val_loss: 0.0352 - val_accuracy: 0.9910\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 33s 78ms/step - loss: 0.0557 - accuracy: 0.9829 - val_loss: 0.0362 - val_accuracy: 0.9903\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 34s 80ms/step - loss: 0.0478 - accuracy: 0.9847 - val_loss: 0.0353 - val_accuracy: 0.9905\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0470 - accuracy: 0.9849 - val_loss: 0.0336 - val_accuracy: 0.9900\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 33s 77ms/step - loss: 0.0431 - accuracy: 0.9857 - val_loss: 0.0332 - val_accuracy: 0.9912\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.0400 - accuracy: 0.9874 - val_loss: 0.0317 - val_accuracy: 0.9915\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 34s 80ms/step - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.0337 - val_accuracy: 0.9917\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0365 - accuracy: 0.9882 - val_loss: 0.0316 - val_accuracy: 0.9905\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.0293 - val_accuracy: 0.9928\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.0327 - accuracy: 0.9893 - val_loss: 0.0343 - val_accuracy: 0.9915\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 34s 80ms/step - loss: 0.0313 - accuracy: 0.9900 - val_loss: 0.0338 - val_accuracy: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc0a3fc7220>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.025992611423134804\n",
      "Test accuracy: 0.9911999702453613\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 28\n",
      "Height: 28\n",
      "Изображение черно-белое.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image = '/home/misha/Изображения/0.png'\n",
    "# predictions = model.predict(np.array([your_image]))\n",
    "# predicted_digit = np.argmax(predictions)\n",
    "# print(\"Predicted digit:\", predicted_digit)\n",
    "# plt.imshow('/home/misha/Изображения/0.png', cmap='binary')\n",
    "# plt.imshow(x_test[12], cmap='binary')\n",
    "# plt.axis('off')\n",
    "# print(y_test[12])\n",
    "\n",
    "# Загрузите ваше изображение и измените его размер до 28x28 пикселей\n",
    "image = Image.open('/home/misha/Изображения/44.png')  # Замените \"your_image.jpg\" на путь к вашему изображению\n",
    "image = image.resize((28, 28))\n",
    "# Получим размер изображения\n",
    "width, height = image.size\n",
    "\n",
    "# Выведем размер изображения\n",
    "print(\"Width:\", width)\n",
    "print(\"Height:\", height)\n",
    "\n",
    "# Загрузите изображение\n",
    "image = cv2.imread('/home/misha/Изображения/44.png', cv2.IMREAD_UNCHANGED)  # Замените путь на ваш путь к изображению\n",
    "\n",
    "# Проверьте количество каналов\n",
    "num_channels = image.shape[2] if len(image.shape) == 3 else 1\n",
    "\n",
    "if num_channels == 1:\n",
    "    print(\"Изображение черно-белое.\")\n",
    "else:\n",
    "    print(\"Изображение цветное.\")\n",
    "\n",
    "# Загрузите цветное изображение\n",
    "# image = cv2.imread('/home/misha/Изображения/5.png', cv2.IMREAD_COLOR)  # Замените путь на ваш путь к изображению\n",
    "\n",
    "# Преобразуйте изображение в черно-белое\n",
    "# gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Сохраните черно-белое изображение\n",
    "# cv2.imwrite('/home/misha/Изображения/05.png', gray_image)\n",
    "\n",
    "# Загрузите изображение\n",
    "# image = cv2.imread('/home/misha/Изображения/05.png', cv2.IMREAD_UNCHANGED)  # Замените путь на ваш путь к изображению\n",
    "\n",
    "# Проверьте количество каналов\n",
    "# num_channels = image.shape[2] if len(image.shape) == 3 else 1\n",
    "\n",
    "# if num_channels == 1:\n",
    "#     print(\"Изображение черно-белое.\")\n",
    "# else:\n",
    "#     print(\"Изображение цветное.\")\n",
    "\n",
    "# Загрузите ваше изображение\n",
    "image = Image.open(\"/home/misha/Изображения/44.png\")\n",
    "\n",
    "# Преобразуйте изображение в формат NumPy и выполните масштабирование\n",
    "your_image = np.array(image)  # Преобразование в NumPy-массив\n",
    "your_image = your_image.astype(\"float32\") / 255  # Масштабирование пикселей в диапазон [0, 1]\n",
    "\n",
    "# Убедитесь, что изображение имеет размерность (28, 28, 1)\n",
    "# your_image = your_image.reshape((28, 28, 1))\n",
    "your_image = np.expand_dims(your_image, axis=-1)\n",
    "\n",
    "plt.imshow(image, cmap='binary')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "Predicted digit: 4\n"
     ]
    }
   ],
   "source": [
    "# Выполните инференс\n",
    "predictions = model.predict(np.array([your_image]))\n",
    "\n",
    "# Результат predictions будет вероятностным распределением по классам.\n",
    "# Для получения конкретного числа (предсказанной цифры) можно воспользоваться:\n",
    "predicted_digit = np.argmax(predictions)\n",
    "\n",
    "print(\"Predicted digit:\", predicted_digit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
